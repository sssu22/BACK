import pandas as pd
import numpy as np
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.cross_validation import random_train_test_split
from lightfm.evaluation import precision_at_k
from sklearn.feature_extraction.text import TfidfVectorizer
import os

# --- 1. ë°ì´í„° ë¡œë”© ---
print("="*20)
print("ğŸ” 1. ë°ì´í„° ë¡œë”© ì‹œì‘")
base_path = os.path.dirname(os.path.abspath(__file__))
interactions_path = os.path.join(base_path, 'trend_recommend_scores.csv')
trends_path = os.path.join(base_path, 'all_trends.csv')

try:
    interactions_df = pd.read_csv(interactions_path)
    trends_df = pd.read_csv(trends_path)
    print(f"  - 'trend_recommend_scores.csv' ë¡œë“œ ì™„ë£Œ (í–‰: {len(interactions_df)}, ì—´: {len(interactions_df.columns)})")
    print(f"  - 'all_trends.csv' ë¡œë“œ ì™„ë£Œ (í–‰: {len(trends_df)}, ì—´: {len(trends_df.columns)})")
    if len(interactions_df) == 0 or len(trends_df) == 0:
        print("âŒ ì˜¤ë¥˜: ì…ë ¥ CSV íŒŒì¼ ì¤‘ í•˜ë‚˜ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    # print("\n[interactions_df ìƒ˜í”Œ]\n", interactions_df.head())
    # print("\n[trends_df ìƒ˜í”Œ]\n", trends_df.head())
except FileNotFoundError as e:
    print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit()

# --- 2. LightFM ë°ì´í„°ì…‹ êµ¬ì„± ---
print("\n" + "="*20)
print("ğŸ“Š 2. LightFM ë°ì´í„°ì…‹ êµ¬ì„± ì‹œì‘")
dataset = Dataset()
dataset.fit(
    users=interactions_df['user_id'],
    items=trends_df['trend_id']
)

# ì•„ì´í…œ í”¼ì²˜ ì¤€ë¹„
trends_df['cat_feat'] = 'category:' + trends_df['category'].astype(str)
trends_df['title_feat'] = 'title:' + trends_df['title'].astype(str)
tfidf = TfidfVectorizer(max_features=50)
desc_tfidf = tfidf.fit_transform(trends_df['description'].fillna(""))
all_features = set(trends_df['cat_feat']) | set(trends_df['title_feat']) | set(tfidf.get_feature_names_out())
dataset.fit_partial(items=trends_df['trend_id'], item_features=all_features)

# ìƒí˜¸ì‘ìš© í–‰ë ¬ ìƒì„±
(interactions, _) = dataset.build_interactions([
    (row['user_id'], row['trend_id'], row['score']) for _, row in interactions_df.iterrows()
])

num_users, num_items = dataset.interactions_shape()
print(f"  - ì´ ìœ ì € ìˆ˜: {num_users}")
print(f"  - ì´ ì•„ì´í…œ ìˆ˜: {num_items}")
print(f"  - ì´ ìƒí˜¸ì‘ìš©(ì ìˆ˜) ìˆ˜: {interactions.nnz}")

if interactions.nnz == 0:
    print("âŒ ì˜¤ë¥˜: ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# ì•„ì´í…œ í”¼ì²˜ í–‰ë ¬ ìƒì„±
trends_df['features'] = trends_df.apply(
    lambda row: [row['cat_feat'], row['title_feat']] + [
        w for w in tfidf.get_feature_names_out() if pd.notna(row['description']) and w in row['description']
    ],
    axis=1
)
item_features = dataset.build_item_features(
    ((row['trend_id'], row['features']) for _, row in trends_df.iterrows())
)

# --- 3. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  ---
print("\n" + "="*20)
print("ğŸš‚ 3. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  ì‹œì‘")
train, test = random_train_test_split(interactions, test_percentage=0.2, random_state=42)
print(f"  - í•™ìŠµ ë°ì´í„° ìƒí˜¸ì‘ìš© ìˆ˜: {train.nnz}")
print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒí˜¸ì‘ìš© ìˆ˜: {test.nnz}")

# --- 4. ëª¨ë¸ í•™ìŠµ ---
print("\n" + "="*20)
print("ğŸ§  4. ëª¨ë¸ í•™ìŠµ ì‹œì‘")
model = LightFM(loss='warp')
# ìœˆë„ìš°ì—ì„œëŠ” num_threadsê°€ ê²½ê³ ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ 1ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥
model.fit(train, item_features=item_features, epochs=10, num_threads=1)
print("  - ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

# --- 5. ëª¨ë¸ í‰ê°€ (Precision@k) ---
print("\n" + "="*20)
print("ğŸ“ˆ 5. ëª¨ë¸ í‰ê°€ ì‹œì‘")
try:
    if test.nnz == 0:
        print("  - Precision@3: (ìŠ¤í‚µ) í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        prec = precision_at_k(model, test, item_features=item_features, k=3)
        print(f"  - Precision@3: {prec.mean():.4f}")
except Exception as e:
    print(f"  - Precision@3 ê³„ì‚°ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤. ì´ìœ : {e}")

# --- 6. ì¶”ì²œ ìƒì„± ---
print("\n" + "="*20)
print("ğŸ’¡ 6. ì¶”ì²œ ìƒì„± ì‹œì‘")
user_id_map, _, item_id_map, _ = dataset.mapping()
user_id_reverse = {v: k for k, v in user_id_map.items()}
item_id_reverse = {v: k for k, v in item_id_map.items()}

recommendations = []
num_items = len(item_id_map)
num_users = len(user_id_map)

print(f"  - ì¶”ì²œ ëŒ€ìƒ ìœ ì € ìˆ˜: {num_users}, ì•„ì´í…œ ìˆ˜: {num_items}")

if num_items == 0 or num_users == 0:
    print("  - ì¶”ì²œ ë¶ˆê°€: ì‚¬ìš©ì ë˜ëŠ” ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    for user_inner_id in range(num_users):
        user_external_id = user_id_reverse[user_inner_id]
        scores = model.predict(user_inner_id, np.arange(num_items), item_features=item_features)

        train_csr = train.tocsr()
        known_items = set(train_csr[user_inner_id].indices) if train_csr.shape[0] > user_inner_id else set()

        unseen = [(item, score) for item, score in enumerate(scores) if item not in known_items]

        if not unseen:
            continue

        top_items = sorted(unseen, key=lambda x: -x[1])[:3]
        for item_inner_id, score in top_items:
            trend_id = item_id_reverse[item_inner_id]
            recommendations.append((user_external_id, trend_id, float(score)))

# --- 7. ê²°ê³¼ ì €ì¥ ---
print("\n" + "="*20)
print("ğŸ’¾ 7. ê²°ê³¼ ì €ì¥ ì‹œì‘")
recommend_df = pd.DataFrame(recommendations, columns=['user_id', 'trend_id', 'score'])
output_path = os.path.join(base_path, 'recommended_trends.csv')
recommend_df.to_csv(output_path, index=False)
print(f"âœ… ì¶”ì²œ ì™„ë£Œ. '{output_path}' ì €ì¥ë¨. (í–‰ ìˆ˜: {len(recommend_df)})")
print("="*20)